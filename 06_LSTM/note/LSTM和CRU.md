## 解决RNN所存在不总

### RNN的上下文传递所存在梯度消失和梯度爆炸问题
1) tanh的反向传播,不断缩小梯度, 容易导致梯度退化(ReLu一定程度可减弱此现象)
2) MatMul可能不断增大梯度引起梯度爆炸, 也可能减少梯度导致梯度消失
3) 解决办法 -> 梯度裁剪

### 从根源上解决RNN的梯度问题-> LSTM, 其存在四种门
1) 输出门, Sigmoid(x_{t-1} * W^{xo} + h_{t-1} * W^{ho} + b^{o}) X tanh(c_{t})
2) 遗忘门 
3) 学习门
4) 输入门
5) LSTM中的门运算的哈达玛积抑制模型的梯度问题

### 效果更好的LSTM
1) 对PTB数据集, 采用2~4层LSTM
2) 并采用Dropout放置模型的过拟合
3) 参数共享 (Embedding层与Affine层共享参数/ 加速收敛且精度提高)